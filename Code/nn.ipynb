{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4chOyDYSsN6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "uHKo3xgf5ByT",
        "colab_type": "code",
        "outputId": "bcc2e226-e147-4d6e-db64-5bbbc9cf766f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os, glob\n",
        "import pickle\n",
        "import os.path\n",
        "import numpy as np\n",
        "#from sklearn.preprocessing import Imputer\n",
        "\n",
        "def preprocess_data(basepath, infile, outfile, wrt):\n",
        "    headers = [\"timestamp\", \"activityid\", \"heartrate\", \"imu1temp\", \"imu1ac1_x\", \"imu1ac1_y\", \"imu1ac1_z\", \"imu1ac2_x\", \"imu1ac2_y\", \"imu1ac2_z\",\n",
        "               \"imu1gy1_x\", \"imu1gy1_y\", \"imu1gy1_z\", \"imu1mag1_x\", \"imu1mag1_y\", \"imu1mag1_z\", \"inv11\", \"inv12\", \"inv13\", \"inv14\", \"imu2temp\",\n",
        "               \"imu2ac1_x\", \"imu2ac1_y\", \"imu2ac1_z\", \"imu2ac2_x\", \"imu2ac2_y\", \"imu2ac2_z\", \"imu2gy1_x\", \"imu2gy1_y\", \"imu2gy1_z\", \"imu2mag1_x\",\n",
        "               \"imu2mag1_y\", \"imu2mag1_z\", \"inv21\", \"inv22\", \"inv23\", \"inv24\", \"imu3temp\", \"imu3ac1_x\", \"imu3ac1_y\", \"imu3ac1_z\", \"imu3ac2_x\",\n",
        "               \"imu3ac2_y\", \"imu3ac2_z\", \"imu3gy1_x\", \"imu3gy1_y\", \"imu3gy1_z\", \"imu3mag1_x\", \"imu3mag1_y\", \"imu3mag1_z\", \"inv31\", \"inv32\", \"inv33\",\n",
        "               \"inv34\"]\n",
        "    subject = pd.read_csv(basepath + infile, sep = '\\s+', names = headers)\n",
        "    drop_columns = [\"inv11\", \"inv12\", \"inv13\", \"inv14\", \"inv21\", \"inv22\", \"inv23\", \"inv24\", \"inv31\", \"inv32\", \"inv33\", \"inv34\", \"imu1ac2_x\", \n",
        "                    \"imu1ac2_y\", \"imu1ac2_z\", \"imu2ac2_x\", \"imu2ac2_y\", \"imu2ac2_z\", \"imu3ac2_x\", \"imu3ac2_y\", \"imu3ac2_z\"]\n",
        "    \n",
        "    \n",
        "    #Interpolate nans\n",
        "    subject = subject.astype(float).interpolate(method = 'linear', limit_direction = 'forward', axis = 0)\n",
        "    #imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
        "    #subject = imp.fit_transform(subject)\n",
        "    #subject = pd.DataFrame(subject)\n",
        "    #subject.columns = headers\n",
        "    subject = subject.drop(drop_columns, axis = 1)\n",
        "    subject = subject[subject.activityid != 0]\n",
        "    \n",
        "    if wrt == 'subject':\n",
        "        target = subject['activityid']\n",
        "        subject = subject.drop(['activityid'], axis = 1)\n",
        "        subject['subjectid'] = int(infile.split('.')[0][7:])\n",
        "        \n",
        "    if wrt == 'activity':\n",
        "        target = infile.split('.')[0][7:]\n",
        "        \n",
        "    subject_data = {'data': subject, 'target': target}\n",
        "    \n",
        "    #Store processed data into pickle file  \n",
        "    if wrt == 'subject':\n",
        "        with open(outfile, 'wb') as file:\n",
        "            pickle.dump(subject_data, file)\n",
        "    \n",
        "    elif wrt == 'activity':\n",
        "        activities = subject.activityid.unique()\n",
        "        for activity in activities:\n",
        "            activity_df = subject.loc[subject['activityid'] == activity]\n",
        "            activity_df.drop(['activityid'], axis = 1)\n",
        "            rows_count = activity_df.shape[0]\n",
        "            activity_target_list = [target] * rows_count\n",
        "            index = np.array(list(range(rows_count)))\n",
        "            activity_target = pd.Series(activity_target_list, index.tolist())\n",
        "            activity_data = {'data': activity_df, 'target': activity_target}\n",
        "            \n",
        "            if os.path.exists(basepath + 'activity' + str(int(activity)) + '.pkl'):\n",
        "                activity_file = open(basepath + 'activity' + str(int(activity)) + '.pkl', 'rb')\n",
        "                act = pickle.load(activity_file)\n",
        "                rows = act['data'].shape[0]\n",
        "                act['data'] = act['data'].append(activity_df)\n",
        "                index = index + rows\n",
        "                act['target'] = act['target'].append(activity_target)\n",
        "                activity_data = act\n",
        "            \n",
        "            with open('activity' + str(int(activity)) + '.pkl', 'wb') as file:\n",
        "                    pickle.dump(activity_data, file)\n",
        "        \n",
        "#basepath = os.path.abspath('PAMAP2_Dataset/Protocol/')\n",
        "#basepath = 'F:/Study/2nd_Semester/AML/Project/Data/PAMAP2_Dataset/Protocol'\n",
        "basepath = '/content'\n",
        "\n",
        "os.chdir(basepath)\n",
        "data_files = glob.glob('*.dat')\n",
        "old_pickle_files = glob.glob('*.pkl')\n",
        "\n",
        "for oldfile in old_pickle_files:\n",
        "    os.remove(oldfile)\n",
        "\n",
        "for infile in data_files:\n",
        "    print(infile)\n",
        "    outfile = infile.split('.')[0] + '.pkl'\n",
        "    preprocess_data(basepath + '/', infile, outfile, 'subject')\n",
        "    preprocess_data(basepath + '/', infile, outfile, 'activity')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subject103.dat\n",
            "subject102.dat\n",
            "subject101.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hp1CvJkusTfG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Windowing"
      ]
    },
    {
      "metadata": {
        "id": "mRv_DSrV5Ig0",
        "colab_type": "code",
        "outputId": "cb94ddb4-9e56-4401-91aa-c21053576f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, glob\n",
        "\n",
        "def downsample(x,factor):\n",
        "    n = int(x.shape[0]/factor)*factor\n",
        "    d1 = x[:n].values.reshape(-1, factor, x.shape[1]).mean(1)\n",
        "    if x.shape[0] % n == 0: dfn = pd.DataFrame(d1)\n",
        "    else:\n",
        "        d2 = x[n:].values.mean(axis = 0).reshape(1,x.shape[1])\n",
        "        dfn = pd.DataFrame(np.concatenate((d1,d2),axis = 0))\n",
        "    dfn.columns = x.columns\n",
        "    return dfn\n",
        "    \n",
        "def window_stack(a, width, stepsize=1):\n",
        "    target = a['target'].iloc[0]\n",
        "    a = a.drop(['target'], axis = 1)\n",
        "    a = downsample(a, 10)\n",
        "    a = a.drop(['timestamp'], axis = 1)\n",
        "    if a.shape[0] < width:\n",
        "        return '', pd.DataFrame()\n",
        "    return target, np.hstack( a[i:1+i-width or None:stepsize] for i in range(0,width))\n",
        "\n",
        "def getWIndowedDataInContinuousChunks(dataframe):\n",
        "    new_dataframe = pd.DataFrame()\n",
        "    startIdx = 0\n",
        "    idx = startIdx\n",
        "    size = dataframe.shape[0]\n",
        "    if size > 489 and round(dataframe.values[0,0] + ((size - 1) * 0.01), 2) == dataframe.values[-1,0]:\n",
        "        target, df = window_stack(dataframe, 50)\n",
        "        df =  pd.DataFrame(df)\n",
        "        df['target'] = target\n",
        "        return df\n",
        "    while idx < size - 1:\n",
        "        if (dataframe['timestamp'].index[idx+1] - dataframe['timestamp'].index[idx]) == 1:\n",
        "            idx += 1\n",
        "        else:\n",
        "            start = dataframe['timestamp'].index[startIdx]\n",
        "            end = dataframe['timestamp'].index[idx]\n",
        "            df = dataframe.loc[start : end - 1, : ]\n",
        "            target, df = window_stack(df, 50)\n",
        "            df = pd.DataFrame(df)\n",
        "            startIdx = idx + 1\n",
        "            idx = startIdx\n",
        "            if df.shape[0] > 0:\n",
        "                df['target'] = target\n",
        "                new_dataframe = new_dataframe.append(df)\n",
        "    return new_dataframe\n",
        "\n",
        "def getChunk(file, outfile):\n",
        "    new_df = pd.DataFrame()\n",
        "    pklFile = open(file, 'rb')\n",
        "    data_from_pickle = pickle.load(pklFile)\n",
        "    target = data_from_pickle['target']\n",
        "    data = data_from_pickle['data']\n",
        "    if file[0] == 's':\n",
        "        data = data.drop(['subjectid'], axis = 1)\n",
        "    elif file[0] == 'a':\n",
        "        data = data.drop(['activityid'], axis = 1)\n",
        "    groups = target.unique()\n",
        "    data['target'] = target.values\n",
        "    outdf = pd.DataFrame()\n",
        "    for group in groups:\n",
        "        df = data.loc[data['target'] == group]\n",
        "        df = df.sort_values(by=['timestamp'])\n",
        "        df = getWIndowedDataInContinuousChunks(df)\n",
        "        new_df = new_df.append(df)\n",
        "    outdf = outdf.append(new_df)\n",
        "    with open(outfile, 'wb') as file:\n",
        "            pickle.dump(outdf, file)\n",
        "\n",
        "#basepath = os.path.abspath('../../Data/PAMAP2_Dataset/Protocol/')\n",
        "#basepath = 'F:/Study/2nd_Semester/AML/Project/Data/PAMAP2_Dataset/Protocol'\n",
        "basepath = '/content'\n",
        "\n",
        "os.chdir(basepath)\n",
        "\n",
        "old_pickle_files = glob.glob('windowed*.pkl')\n",
        "for oldfile in old_pickle_files:\n",
        "    os.remove(oldfile)\n",
        "\n",
        "pickle_files = glob.glob('*.pkl')\n",
        "\n",
        "for file in pickle_files:\n",
        "    print('Processing', file)\n",
        "    outfile = 'windowed_' + file \n",
        "    getChunk(file, outfile)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing activity7.pkl\n",
            "Processing activity16.pkl\n",
            "Processing activity5.pkl\n",
            "Processing activity4.pkl\n",
            "Processing activity12.pkl\n",
            "Processing subject101.pkl\n",
            "Processing subject102.pkl\n",
            "Processing activity6.pkl\n",
            "Processing subject103.pkl\n",
            "Processing activity1.pkl\n",
            "Processing activity13.pkl\n",
            "Processing activity2.pkl\n",
            "Processing activity3.pkl\n",
            "Processing activity24.pkl\n",
            "Processing activity17.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GaJltiVrsXiR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LOSO and nn Model training and testing"
      ]
    },
    {
      "metadata": {
        "id": "hkKWVtiUD8rd",
        "colab_type": "code",
        "outputId": "728245ce-65f1-49b4-bda6-326192c9f717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "cell_type": "code",
      "source": [
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess_dataframe(data,split=False):\n",
        "    y=data['target'].values\n",
        "    data=data.drop(['target'],axis=1)\n",
        "    y=y.astype(np.int)\n",
        "    X=data.values\n",
        "    data=None\n",
        "    X=preprocessing.normalize(X)\n",
        "    if split:\n",
        "        return train_test_split(X,y)\n",
        "    else:\n",
        "        return X,y\n",
        "\n",
        "def Run_LOSO():\n",
        "    #basepath = os.path.abspath('../Data/PAMAP2_Dataset/Protocol/')\n",
        "    #basepath = 'F:/Study/2nd_Semester/AML/Project/Data/PAMAP2_Dataset/Protocol'\n",
        "    basepath = '/content'\n",
        "    os.chdir(basepath)\n",
        "    subject_files = glob.glob('windowed_subject*.pkl')\n",
        "    for i in range(len(subject_files)):\n",
        "        print(subject_files[i])\n",
        "        temp_file = np.copy(subject_files).tolist()\n",
        "        pklfile = open(subject_files[i], 'rb')\n",
        "        test_data = pickle.load(pklfile)\n",
        "        X_test,y_test=preprocess_dataframe(test_data)\n",
        "        temp_file.remove(subject_files[i])\n",
        "        train_data = pd.DataFrame()\n",
        "        for file in temp_file:\n",
        "            pklfile = open(file, 'rb')\n",
        "            data_from_pickle = pickle.load(pklfile)\n",
        "            train_data = train_data.append(data_from_pickle) \n",
        "        X_train,y_train = preprocess_dataframe(train_data)\n",
        "        \n",
        "        sess = tf.Session()\n",
        "        X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "        \n",
        "        X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "        y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
        "        \n",
        "        X_train = sess.run(X_train)\n",
        "        y_train = tf.cast(y_train, tf.int32)\n",
        "        y_train = tf.one_hot(y_train, len(np.unique(y_train)))\n",
        "        y_train = sess.run(y_train)\n",
        "        \n",
        "        X_test = sess.run(X_test)\n",
        "        y_test = tf.cast(y_test, tf.int32)\n",
        "        y_test = tf.one_hot(y_test, len(np.unique(y_test)))\n",
        "        y_test = sess.run(y_test)\n",
        "        \n",
        "        #print(type(X_train), type(X_test), type(y_train), type(y_test))\n",
        "        nn(X_train, y_train, X_test, y_test)\n",
        "\n",
        "def nn(X_train, y_train, X_test, y_test):\n",
        "    print('inside nn')\n",
        "    training_epochs = 5\n",
        "    n_neurons_in_h1 = 10\n",
        "    n_neurons_in_h2 = 10\n",
        "    learning_rate = 0.01\n",
        "    n_features = X_train.shape[1]\n",
        "    n_classes = len(np.unique(y_train))\n",
        "    \n",
        "    X = tf.placeholder(tf.float32, [None, n_features], name='features')\n",
        "    Y = tf.placeholder(tf.float32, [None, n_classes], name='labels')        \n",
        "    \n",
        "    W1 = tf.Variable(tf.truncated_normal([n_features, n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)), name='weights1')\n",
        "    b1 = tf.Variable(tf.truncated_normal([n_neurons_in_h1],mean=0, stddev=1 / np.sqrt(n_features)), name='biases1')\n",
        "    y1 = tf.nn.tanh((tf.matmul(X, W1)+b1), name='activationLayer1')\n",
        "    \n",
        "    #network parameters(weights and biases) are set and initialized(Layer2)\n",
        "    W2 = tf.Variable(tf.random_normal([n_neurons_in_h1, n_neurons_in_h2],mean=0,stddev=1/np.sqrt(n_features)),name='weights2')\n",
        "    b2 = tf.Variable(tf.random_normal([n_neurons_in_h2],mean=0,stddev=1/np.sqrt(n_features)),name='biases2')\n",
        "    #activation function(sigmoid)\n",
        "    y2 = tf.nn.sigmoid((tf.matmul(y1,W2)+b2),name='activationLayer2')\n",
        "    \n",
        "    #output layer weights and biasies\n",
        "    Wo = tf.Variable(tf.random_normal([n_neurons_in_h2, n_classes], mean=0, stddev=1/np.sqrt(n_features)), name='weightsOut')\n",
        "    bo = tf.Variable(tf.random_normal([n_classes], mean=0, stddev=1/np.sqrt(n_features)), name='biasesOut')\n",
        "    #activation function(softmax)\n",
        "    a = tf.nn.softmax((tf.matmul(y2, Wo) + bo), name='activationOutputLayer')\n",
        "    \n",
        "    #cost function\n",
        "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(a),reduction_indices=[1]))\n",
        "    \n",
        "    #optimizer\n",
        "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
        "    \n",
        "    #compare predicted value from network with the expected value/target\n",
        "    correct_prediction = tf.equal(tf.argmax(a, 1), tf.argmax(Y, 1))\n",
        "\n",
        "    #accuracy determination\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")\n",
        "    \n",
        "    # initialization of all variables\n",
        "    initial = tf.global_variables_initializer()\n",
        "    \n",
        "    #creating a session\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(initial)\n",
        "        merged_summary = tf.summary.merge_all()\n",
        "        \n",
        "        # training loop over the number of epoches\n",
        "        batchsize=10000\n",
        "        for epoch in range(training_epochs):\n",
        "            if len(X_train)%batchsize == 0:\n",
        "                Totalbatches = int(len(X_train)/batchsize)\n",
        "            else:\n",
        "                Totalbatches = int(len(X_train)/batchsize + 1)\n",
        "                \n",
        "            for i in range(Totalbatches):\n",
        "                start=i\n",
        "                end=i+batchsize\n",
        "                x_batch=X_train[start:end]\n",
        "                y_batch=y_train[start:end]\n",
        "                \n",
        "                # feeding training data/examples\n",
        "                sess.run(train_step, feed_dict={X:x_batch , Y:y_batch})\n",
        "                i+=batchsize\n",
        "            \n",
        "            # feeding testing data to determine model accuracy\n",
        "            y_pred = sess.run(tf.argmax(a, 1), feed_dict={X: X_test})\n",
        "            y_true = sess.run(tf.argmax(y_test, 1))\n",
        "\n",
        "            acc = sess.run([accuracy], feed_dict={X: X_test, Y: y_test})\n",
        "\n",
        "            # print accuracy for each epoch\n",
        "            print('epoch',epoch, acc)\n",
        "            print ('---------------')\n",
        "            #print(y_pred, y_true)\n",
        "    \n",
        "Run_LOSO()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "windowed_subject103.pkl\n",
            "inside nn\n",
            "epoch 0 [1.0]\n",
            "---------------\n",
            "epoch 1 [1.0]\n",
            "---------------\n",
            "epoch 2 [1.0]\n",
            "---------------\n",
            "epoch 3 [1.0]\n",
            "---------------\n",
            "epoch 4 [1.0]\n",
            "---------------\n",
            "windowed_subject102.pkl\n",
            "inside nn\n",
            "epoch 0 [1.0]\n",
            "---------------\n",
            "epoch 1 [1.0]\n",
            "---------------\n",
            "epoch 2 [1.0]\n",
            "---------------\n",
            "epoch 3 [1.0]\n",
            "---------------\n",
            "epoch 4 [1.0]\n",
            "---------------\n",
            "windowed_subject101.pkl\n",
            "inside nn\n",
            "epoch 0 [1.0]\n",
            "---------------\n",
            "epoch 1 [1.0]\n",
            "---------------\n",
            "epoch 2 [1.0]\n",
            "---------------\n",
            "epoch 3 [1.0]\n",
            "---------------\n",
            "epoch 4 [1.0]\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}